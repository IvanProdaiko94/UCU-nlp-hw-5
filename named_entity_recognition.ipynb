{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implement functionality to read and process NER 2003 English Shared Task data in CoNNL file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_folder(folder, ext):\n",
    "    return [\n",
    "        read_file(os.path.join(folder, filename)) for filename in sorted(os.listdir(folder)) if filename.endswith(ext)\n",
    "    ]\n",
    "\n",
    "def read_connl_to_df(folder, ext):\n",
    "    result = []\n",
    "    offest = 0\n",
    "    for txt in read_folder(folder, ext):\n",
    "        rows = []\n",
    "        sentences = txt.split('\\n\\n')\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for word in sentence.split('\\n'):\n",
    "                row = word.split(' ')\n",
    "                row.append(int(offest + i))\n",
    "                rows.append(row)\n",
    "        result.append(pd.DataFrame(rows))\n",
    "        offest = len(sentences)\n",
    "    return result\n",
    "\n",
    "def set_connl_df_naming(df):\n",
    "    df.columns = ['word', 'part_of_speech', 'chunk', 'tag', 'sentence_id']\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3685\n",
      "14988\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>chunk</th>\n",
       "      <th>tag</th>\n",
       "      <th>sentence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOCCER</td>\n",
       "      <td>NN</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JAPAN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GET</td>\n",
       "      <td>VB</td>\n",
       "      <td>B-VP</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LUCKY</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WIN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHINA</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IN</td>\n",
       "      <td>IN</td>\n",
       "      <td>B-PP</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SURPRISE</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word part_of_speech chunk    tag  sentence_id\n",
       "0    SOCCER             NN  B-NP      O          1.0\n",
       "1         -              :     O      O          1.0\n",
       "2     JAPAN            NNP  B-NP  B-LOC          1.0\n",
       "3       GET             VB  B-VP      O          1.0\n",
       "4     LUCKY            NNP  B-NP      O          1.0\n",
       "5       WIN            NNP  I-NP      O          1.0\n",
       "6         ,              ,     O      O          1.0\n",
       "7     CHINA            NNP  B-NP  B-PER          1.0\n",
       "8        IN             IN  B-PP      O          1.0\n",
       "9  SURPRISE             DT  B-NP      O          1.0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test, train = [set_connl_df_naming(df.iloc[1:]) for df in read_connl_to_df('./dataset/', '.txt')]\n",
    "df = test.append(train)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement 3 strategies for loading the embeddings\n",
    "\n",
    "1) load the embeddings for original capitalization of words. If embedding for this word doesn’t exists, associate it with UNKNOWN embedding (5% of score).\n",
    "\n",
    "2) load the embeddings for lowercased capitalization of words. If embedding for this lowercased word doesn’t exists, associate it with UNKNOWN embedding (5% of score).\n",
    "\n",
    "3) load the embeddings for original capitalization of words. If embedding for this word doesn’t exists, try to find the embedding for lowercased version and associate it to the word with original capitalization. Otherwise, associate it with UNKNOWN embedding (20% of score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec('./glove.6B.100d.txt', word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STRATEGY(Enum):\n",
    "    CASED = 1\n",
    "    LOWER = 2\n",
    "    FALLBACK_TO_LOWER = 3\n",
    "\n",
    "class W2V:\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def get_vector(self, word):\n",
    "        try:\n",
    "            return self.w2v.get_vector(word)\n",
    "        except KeyError:\n",
    "            return self.w2v.get_vector(\"unk\")\n",
    "        \n",
    "    def get_vector_lowercased(self, word):\n",
    "        try:\n",
    "            return self.w2v.get_vector(str(word).lower())\n",
    "        except KeyError:\n",
    "            return self.w2v.get_vector(\"unk\")\n",
    "        \n",
    "    def get_vector_lowercased_onfail(self, word):\n",
    "        unk = self.w2v.get_vector(\"unk\")\n",
    "        original_case = self.get_vector(word)\n",
    "        \n",
    "        if np.array_equal(unk, original_case):\n",
    "            return self.get_vector_lowercased(word)\n",
    "        \n",
    "        return original_case\n",
    "    \n",
    "    def enreach_df_with_vector_representation_of_words(self, df, strategy, col_in, col_out):\n",
    "        if strategy == STRATEGY.CASED:\n",
    "            df[col_out] = df[col_in].apply(lambda word: self.get_vector(word))\n",
    "        if strategy == STRATEGY.LOWER:\n",
    "            df[col_out] = df[col_in].apply(lambda word: self.get_vector_lowercased(word))\n",
    "        if strategy == STRATEGY.FALLBACK_TO_LOWER:\n",
    "            df[col_out] = df[col_in].apply(lambda word: self.get_vector_lowercased_onfail(word))\n",
    "        return df\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_word2vec_format(path):\n",
    "        embeddings = KeyedVectors.load_word2vec_format(path)\n",
    "        return W2V(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = W2V.load_word2vec_format(word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>chunk</th>\n",
       "      <th>tag</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOCCER</td>\n",
       "      <td>NN</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.027166, -0.1762, -0.19623, 0.33527, 0.06239...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-1.2557, 0.61036, 0.56793, -0.96596, -0.45249...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JAPAN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.027166, -0.1762, -0.19623, 0.33527, 0.06239...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GET</td>\n",
       "      <td>VB</td>\n",
       "      <td>B-VP</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.027166, -0.1762, -0.19623, 0.33527, 0.06239...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LUCKY</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.027166, -0.1762, -0.19623, 0.33527, 0.06239...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word part_of_speech chunk    tag  sentence_id  \\\n",
       "0  SOCCER             NN  B-NP      O          1.0   \n",
       "1       -              :     O      O          1.0   \n",
       "2   JAPAN            NNP  B-NP  B-LOC          1.0   \n",
       "3     GET             VB  B-VP      O          1.0   \n",
       "4   LUCKY            NNP  B-NP      O          1.0   \n",
       "\n",
       "                                                 vec  \n",
       "0  [0.027166, -0.1762, -0.19623, 0.33527, 0.06239...  \n",
       "1  [-1.2557, 0.61036, 0.56793, -0.96596, -0.45249...  \n",
       "2  [0.027166, -0.1762, -0.19623, 0.33527, 0.06239...  \n",
       "3  [0.027166, -0.1762, -0.19623, 0.33527, 0.06239...  \n",
       "4  [0.027166, -0.1762, -0.19623, 0.33527, 0.06239...  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_str_1 = glove.enreach_df_with_vector_representation_of_words(df.copy(deep=True), STRATEGY.CASED, \"word\", \"vec\")\n",
    "df_str_2 = glove.enreach_df_with_vector_representation_of_words(df.copy(deep=True), STRATEGY.LOWER, \"word\", \"vec\")\n",
    "df_str_3 = glove.enreach_df_with_vector_representation_of_words(df.copy(deep=True), STRATEGY.FALLBACK_TO_LOWER, \"word\", \"vec\")\n",
    "\n",
    "df_str_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement training on batches\n",
    "\n",
    "[Article](https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da)\n",
    "\n",
    "[Tutorial](http://cs230.stanford.edu/blog/namedentity/)\n",
    "\n",
    "[DOCS](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-LOC': 1,\n",
       " 'B-PER': 2,\n",
       " 'I-PER': 3,\n",
       " 'I-LOC': 4,\n",
       " 'B-MISC': 5,\n",
       " 'I-MISC': 6,\n",
       " 'B-ORG': 7,\n",
       " 'I-ORG': 8,\n",
       " None: 9}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = {}\n",
    "for i, label in enumerate(df['tag'].unique()):\n",
    "    labels[label] = i\n",
    "    \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vacabulary:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.drop_duplicates('word', keep='first')\n",
    "        self.df = self.df.reset_index()\n",
    "        self.df.set_index(\"word\", inplace=True)\n",
    "    \n",
    "    def get_vec_by_word(self, word):\n",
    "        return self.df.loc[word]['vec']\n",
    "    \n",
    "    def get_index_by_word(self, word):\n",
    "        return self.df.loc[word]['index']\n",
    "    \n",
    "    def get_vec_by_index(self, i):\n",
    "        return self.df.iloc[i]['vec']\n",
    "    \n",
    "    def embedding_matrix(self):\n",
    "        return self.df['vec'].to_numpy(copy=True)\n",
    "    \n",
    "    def get_padding(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreparationPipeline:\n",
    "    @staticmethod\n",
    "    def __to_batches(df, batch_size):\n",
    "        batches = []\n",
    "        offset = 0\n",
    "        for index, row in df.iterrows():\n",
    "            if row['sentence_id'] % batch_size == 0 and \\\n",
    "                index + 1 < len(df) and \\\n",
    "                df.iloc[index + 1]['sentence_id'] % batch_size != 0:\n",
    "                batches.append(df[offset:index+1])\n",
    "                offset = index + 1\n",
    "        return batches\n",
    "    \n",
    "    @staticmethod\n",
    "    def __to_sentences(batch):\n",
    "        result = []\n",
    "        batch.groupby('sentence_id').apply(lambda x: result.append((x['word'].values, x['tag'].values)))\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def __to_indexes(batch, vocab, label_mapping):\n",
    "        max_len = 0\n",
    "        for (words, _) in batch:\n",
    "            if max_len < len(words):\n",
    "                max_len = len(words)\n",
    "                \n",
    "        batch_data = vocab.get_padding() * np.ones((len(batch), max_len))\n",
    "        batch_labels = -1*np.ones((len(batch), max_len))\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            words, labels = batch[i]\n",
    "            cur_len = len(words)\n",
    "            batch_data[i][:cur_len] = np.array([vocab.get_index_by_word(word) for word in words])\n",
    "            batch_labels[i][:cur_len] = np.array([label_mapping[label] for label in labels])\n",
    "        \n",
    "        #since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "\n",
    "        #convert Tensors to Variables\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "        return (batch_data, batch_labels)\n",
    "            \n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare(df, vocab, labels, batch_size):\n",
    "        pp = PreparationPipeline\n",
    "        \n",
    "        batches = [pp.__to_sentences(batch) for batch in pp.__to_batches(test, batch_size)]\n",
    "        \n",
    "        result_batches = []\n",
    "        result_labels  = []\n",
    "        \n",
    "        for batch in batches:\n",
    "            batch_data, batch_labels = pp.__to_indexes(batch, vocab, labels)\n",
    "            result_batches.append(batch_data)\n",
    "            result_labels.append(batch_labels)\n",
    "        \n",
    "        return result_batches, result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_matrix, lstm_hidden_dim, vocab_size, result_size):\n",
    "        super(Net, self).__init__()\n",
    "        #maps each token to an embedding_dim vector\n",
    "        self.embedding = nn.Embedding(\n",
    "            embedding_matrix.size()[0], \n",
    "            embedding_matrix.size()[1],\n",
    "        ).from_pretrained(\n",
    "            embedding_matrix, \n",
    "            freeze=True,\n",
    "        )            \n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        #the LSTM takens embedded sentence\n",
    "        self.lstm = nn.LSTM(embedding_matrix.size()[1], lstm_hidden_dim, num_layers=2, bidirectional=True)\n",
    "\n",
    "        #fc layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(2*lstm_hidden_dim, result_size)\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        print(sentence)\n",
    "        #apply the embedding layer that maps each token to its embedding\n",
    "        sentence = self.embedding(sentence)   # dim: batch_size x batch_max_len x embedding_dim\n",
    "\n",
    "        #run the LSTM along the sentences of length batch_max_len\n",
    "        sentence, _ = self.lstm(sentence)     # dim: batch_size x batch_max_len x lstm_hidden_dim                \n",
    "\n",
    "        #reshape the Variable so that each row contains one token\n",
    "        sentence = sentence.view(-1, sentence.shape[2])  # dim: batch_size*batch_max_len x lstm_hidden_dim\n",
    "\n",
    "        #apply the fully connected layer and obtain the output for each token\n",
    "        sentence = self.fc(sentence)          # dim: batch_size*batch_max_len x num_tags\n",
    "\n",
    "        return F.log_softmax(sentence, dim=1)   # dim: batch_size*batch_max_len x num_tags\n",
    "    \n",
    "    def loss_fn(outputs, labels):\n",
    "        #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "        labels = labels.view(-1)  \n",
    "\n",
    "        #mask out 'PAD' tokens\n",
    "        mask = (labels >= 0).float()\n",
    "\n",
    "        #the number of tokens is the sum of elements in mask\n",
    "        num_tokens = int(torch.sum(mask).data[0])\n",
    "\n",
    "        #pick the values corresponding to labels and multiply by mask\n",
    "        outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
    "\n",
    "        #cross entropy loss for all non 'PAD' tokens\n",
    "        return -torch.sum(outputs) / num_tokens\n",
    "    \n",
    "    def fit(self, train_batches, train_labels, epochs=5):\n",
    "        for i in range(epochs):\n",
    "            print(\"Epoch: {}\".format(i))\n",
    "            \n",
    "            total_loss = 0\n",
    "            for j, batch in enumerate(train_batches):\n",
    "                self.zero_grad()\n",
    "                print(batch.size())\n",
    "                #pass through model, perform backpropagation and updates\n",
    "                output_batch = self.forward(batch)\n",
    "                exspected_labels = train_labels[j]\n",
    "                loss = self.loss_fn(output_batch, exspected_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss\n",
    "                \n",
    "            avg_epoch_loss = np.round((total_loss / len(train_batches)).item(), 3)\n",
    "            \n",
    "            print(\"Total epoch loss: {}\".format(total_loss))\n",
    "            print(\"Avg epoch loss: {}\".format(avg_epoch_loss))\n",
    "    \n",
    "    def predict(self, test_batches):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vacabulary(df_str_1)\n",
    "matrix = vocab.embedding_matrix()\n",
    "result = []\n",
    "for i, arr in enumerate(matrix):\n",
    "    result.append(torch.as_tensor(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches, train_labels = PreparationPipeline.prepare(df_str_1[:len(train)], vocab, labels, BATCH_SIZE)\n",
    "test_batches, test_labels = PreparationPipeline.prepare(df_str_1[len(train):], vocab, labels, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "torch.Size([65, 72])\n",
      "tensor([[    0,     1,     2,  ..., 27318, 27318, 27318],\n",
      "        [    0,     1,     2,  ..., 27318, 27318, 27318],\n",
      "        [   12,    13, 27318,  ..., 27318, 27318, 27318],\n",
      "        ...,\n",
      "        [ 1136,  1137,  1138,  ..., 27318, 27318, 27318],\n",
      "        [ 1143,  1144,  1145,  ..., 27318, 27318, 27318],\n",
      "        [ 1150,  1151,  1152,  ..., 27318, 27318, 27318]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index out of range at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:193",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-434-35e99ebf92cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-432-ddecaea947db>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_batches, train_labels, epochs)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;31m#pass through model, perform backpropagation and updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0moutput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mexspected_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexspected_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-432-ddecaea947db>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#apply the embedding layer that maps each token to its embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# dim: batch_size x batch_max_len x embedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#run the LSTM along the sentences of length batch_max_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m         return F.embedding(\n\u001b[1;32m    116\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index out of range at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:193"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = glove.w2v.vector_size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "net = Net(\n",
    "    torch.stack(result),\n",
    "    HIDDEN_DIM,\n",
    "    vocab.get_size(),\n",
    "    len(labels)\n",
    ")\n",
    "\n",
    "net.fit(train_batches, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implement the calculation of token-level Precision / Recall / F1 / F0.5 scores for all classes in average. \n",
    "\n",
    "# IMPORTANT! Please, implement “micro-average” approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provide the report the performances (F1 and F0.5 scores) on the dev / test subsets w.r.t epoch number during the training for the first 5 epochs for each strategy of loading the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
